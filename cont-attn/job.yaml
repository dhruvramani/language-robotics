apiVersion: v1
kind: Pod
# @dhruvramani - reference file. Rename to `job.yaml`. Search for IMP and edit.
metadata:
  # IMP: Name of the pod. This is the name to use in any interaction with kubectl
  name: langrobo
spec:
  securityContext:
    # IMP: You need to specify your user ID here. You can get this by running "id -u"
    runAsUser: 1174
  volumes:
  # IMP : The following 3 entries are the NFS mounts available to you. You do not need to
  # modify them. They are for your home folder, and the shared tools and datasets
  # folders.
  - name: dshm #NOTE : Need this for PyTorch DataLoaders
    emptyDir:
      medium: Memory
  - name: home
    persistentVolumeClaim:
      claimName: home
  - name: tools
    persistentVolumeClaim:
      claimName: tools
#  - name: data1
#    persistentVolumeClaim:
#      claimName: data1
#  - name: data2
#    persistentVolumeClaim:
#      claimName: data2
#  - name: data3
#    persistentVolumeClaim:
#      claimName: data3
#  - name: scratch1
#    persistentVolumeClaim:
#      claimName: scratch1
  - name: scratch2
    persistentVolumeClaim:
      claimName: scratch2
  # The following 3 entries make the Nvidia drivers available. You do not need to
  # modify them.
  - hostPath:
      path: /usr/lib/nvidia-driver/bin
    name: nvbin
  - hostPath:
      path: /usr/lib/nvidia-driver
    name: nvlib
  - hostPath:
      path: /usr/lib/
    name: usrlib
  - hostPath:
      path: /usr/bin
    name: bin
  - hostPath:
      path: /lib
    name: lib
  # IMP: You can specify a label indicating a specific model of GPU here when you need it.
  # Leave it commented when learning how to use the cluster.
#  nodeSelector:
#    kubernetes.io/hostname: turing
  # This part specifies the Docker container in which your code will run.
  containers:
  - name: py # container_name The container also has a name but it ususally doesn't matter
    # This is the Docker image on which your container is based. Leave it unchanged.
    image: ubuntu:16.04
    # IMP : This is the command which is run once the container is created. Point it to
    # your code. It is best to encapsulate your commands in a shell script and
    # call that script like below.
    # 
    # Note that all paths have to be according to the filesystem inside the container.
    command: ["/bin/bash", "/storage/home/dhruvramani/language-robotics/cont-attn/run.sh"]
    # This section specifies the resources you ask for.
    resources:
      # For the purposes of using the cluster, the "requests" section should be the
      # same as the "limits" section except for the line for GPU
      limits:
        alpha.kubernetes.io/nvidia-gpu: 2 # IMP : Number of GPUs
        # IMP : RAM can be specified in either MiB (1024x1024 bytes) (ex. "500Mi")
        # or GiB (ex. "4Gi")
        memory: "50Gi"
        cpu: "8" # IMP : Number of CPU cores
      requests:
        memory: "50Gi"
        cpu: "8"
    # IMP : Here, you can specify where you want the above directories to be mounted
    # within the container. It is recommended to use this configuration as
    # it mirrors what is present on the master machine.
    volumeMounts:
    # IMP : Entry for your home folder. Make sure to replace <username> with your
    # username.
    - mountPath: /storage/home/dhruvramani
      name: home
    # Entry for the other folders mentioned above.
    - mountPath: /tools
      name: tools
    - mountPath: /dev/shm
      name: dshm
#    - mountPath: /datasets/data1
#      name: data1
#    - mountPath: /datasets/data2
#      name: data2
#    - mountPath: /datasets/data3
#      name: data3
#    - mountPath: /scratch/scratch1
#      name: scratch1
    - mountPath: /scratch/scratch2
      name: scratch2
    - mountPath: /usr/local/nvidia/bin
      name: nvbin
    - mountPath: /usr/local/nvidia/lib
      name: nvlib
    - mountPath: /usr/lib/
      name: usrlib
    - mountPath: /usr/bin/
      name: bin
    - mountPath: /lib
      name: lib
  restartPolicy: Never
